{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from itertools import chain\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed of all polssible random number generators\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# enable CUDA_LAUNCH_BLOCKING=1 to debug cuda\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_frame():\n",
    "    df = pd.read_csv(\"all_data.csv\")\n",
    "    episodes = df.groupby(\"Episode\")\n",
    "    episode_data = {}\n",
    "    for episode, data in episodes:\n",
    "        episode_data[episode] = [\n",
    "            [row[\"X\"], row[\"Y\"], row[\"Action\"], row[\"Reward\"]]\n",
    "            for index, row in data.iterrows()\n",
    "        ]\n",
    "    return episode_data\n",
    "\n",
    "\n",
    "episode_data = [episode for episode in read_data_frame().values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data and split it into training and testing\n",
    "np.random.shuffle(episode_data)\n",
    "train_data = episode_data[: int(len(episode_data) * 0.8)]\n",
    "test_data = episode_data[int(len(episode_data) * 0.8) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    block_size = 512\n",
    "    start_token = 0\n",
    "    max_grid_size = 50\n",
    "    padding_token = 1\n",
    "    end_token = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    state_min, state_max = 0, 9  # Adjust based on your environment\n",
    "    state_bins = 10\n",
    "    reward_min, reward_max = -1, 1  # Adjust based on your environment\n",
    "    reward_bins = 20\n",
    "    action_dim = 5\n",
    "    n_embd = 128\n",
    "    n_head = 8\n",
    "    n_layer = 8\n",
    "    dropout = 0.2\n",
    "    vocab_size = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    mapper = dict(\n",
    "        {\n",
    "            CFG.start_token: CFG.start_token,\n",
    "            CFG.padding_token: CFG.padding_token,\n",
    "            CFG.end_token: CFG.end_token,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    vocab_size = CFG.max_grid_size * 2 + CFG.action_dim + 2 + 3\n",
    "    offset = 3\n",
    "    coordinate_demention_size = 100  # max grid size\n",
    "    token_counter = 3\n",
    "\n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    # gets one unicode number. It should check if the unicode number is already in the mapper. If not, it should add it. Returns the number.\n",
    "    def get_tokenized_unicode(self, x):\n",
    "        if x not in self.mapper:\n",
    "            self.mapper[x] = self.token_counter\n",
    "            self.token_counter += 1\n",
    "        return self.mapper[x]\n",
    "\n",
    "    def encode(self, x: np.array):\n",
    "        x = np.array(x)\n",
    "        tokens = []\n",
    "        for i in range(0, len(x)):\n",
    "            step = np.array(x[i])\n",
    "            tokens.extend(\n",
    "                [\n",
    "                    step[0] + self.offset,\n",
    "                    step[1] + self.offset + CFG.max_grid_size,\n",
    "                    step[2] + self.offset + CFG.max_grid_size * 2,\n",
    "                    step[3] + self.offset + CFG.max_grid_size * 2 + CFG.action_dim,\n",
    "                ]\n",
    "            )\n",
    "            # step = \",\".join(step.astype(str))\n",
    "            # for i in range(len(step)):\n",
    "            #     unicode_numb = ord(step[i])\n",
    "            #     tokens.append(self.get_tokenized_unicode(unicode_numb))\n",
    "        return [CFG.start_token] + tokens + [CFG.end_token]\n",
    "\n",
    "    # def cut_to_max_len(self, x):\n",
    "    #     tokenized_steps = []\n",
    "    #     length = 0\n",
    "    #     for i in range(0, len(x)):\n",
    "    #         step = x[i]\n",
    "    #         tokenized_step = self.encode([step])\n",
    "    #         if length + len(tokenized_step) > CFG.block_size - 2:\n",
    "    #             return x[:i]\n",
    "    #         tokenized_steps.append(tokenized_step)\n",
    "    #         length += len(tokenized_step)\n",
    "\n",
    "    #     return x\n",
    "\n",
    "    def pad(self, x):\n",
    "        return x + [CFG.padding_token] * (CFG.block_size - len(x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        padded = self.pad(encoded)\n",
    "        if len(padded) != CFG.block_size:\n",
    "            print(\"padding error\")\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, global_history, tokenizer):\n",
    "        self.global_history = global_history\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.global_history)\n",
    "\n",
    "    def crop(self, arr):\n",
    "        if len(arr) > CFG.block_size // 4:\n",
    "            # make sequence CFG.block_size wize by randomly cropping the sequence\n",
    "            start_index = np.random.randint(0, len(arr) - CFG.block_size // 4)\n",
    "            arr = arr[start_index : start_index + CFG.block_size // 4]\n",
    "\n",
    "        take_first = np.random.randint(2, len(arr))\n",
    "        target = arr[-1]\n",
    "        arr = arr[: take_first - 1]\n",
    "        return arr, target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        learning_history = self.global_history[idx]\n",
    "\n",
    "        learning_history, target = self.crop(learning_history)\n",
    "\n",
    "        tokenized = self.tokenizer(learning_history)\n",
    "        tensor = torch.tensor(tokenized, dtype=torch.long)\n",
    "        action = torch.tensor(target[2], dtype=torch.long)\n",
    "        if tensor.shape[0] > CFG.block_size:\n",
    "            print(tensor.shape)\n",
    "\n",
    "        return tensor, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = A3CDataset(episode_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset, batch_size=CFG.batch_size)\n",
    "\n",
    "# for i, batch in enumerate(loader):\n",
    "#     print(i)\n",
    "#     print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3219"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3219"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A3CDataset(train_data, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    A3CDataset(train_data, tokenizer=tokenizer),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    A3CDataset(test_data, tokenizer=tokenizer), batch_size=CFG.batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [tensor([[ 0,  7, 56,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 57,  ...,  1,  1,  1],\n",
      "        [ 0,  7, 56,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 57,  ...,  1,  1,  1],\n",
      "        [ 0,  7, 56,  ...,  1,  1,  1],\n",
      "        [ 0,  7, 56,  ...,  1,  1,  1]]), tensor([2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2,\n",
      "        2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0,\n",
      "        2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"sample: {next(iter(train_data_loader))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find vocabulary size base on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_loader = DataLoader(\n",
    "    A3CDataset(episode_data, tokenizer=tokenizer),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 0\n"
     ]
    }
   ],
   "source": [
    "for x, y in all_data_loader:\n",
    "    # noop\n",
    "    a = 2\n",
    "\n",
    "print(f\"Vocab size: {CFG.vocab_size}\")\n",
    "CFG.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.663621 M parameters\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "# batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "# max_iters = 5000\n",
    "# eval_interval = 500\n",
    "# learning_rate = 3e-4\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# eval_iters = 200\n",
    "# n_embd = 384\n",
    "# n_head = 8\n",
    "# n_layer = 8\n",
    "# dropout = 0.2\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(CFG.n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(CFG.n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(CFG.n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\n",
    "            \"tril\", torch.tril(torch.ones(CFG.block_size, CFG.block_size))\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(CFG.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (\n",
    "            q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        )  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, CFG.n_embd)\n",
    "        self.dropout = nn.Dropout(CFG.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(CFG.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(CFG.vocab_size, CFG.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(CFG.block_size, CFG.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(CFG.n_embd, n_head=CFG.n_head) for _ in range(CFG.n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(CFG.n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(CFG.n_embd, CFG.action_dim)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        # tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=CFG.device)\n",
    "        )  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x[:, -1, :])  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # B, T, C = logits.shape\n",
    "            # logits = logits.view(B, C * T)\n",
    "            # targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(CFG.device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = ADTokenizer(CFG)\n",
    "\n",
    "# train_data_loader = DataLoader(\n",
    "#     A3CDataset(train_data, tokenizer=tokenizer),\n",
    "#     batch_size=CFG.batch_size,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "# test_data_loader = DataLoader(\n",
    "#     A3CDataset(test_data, tokenizer=tokenizer), batch_size=CFG.batch_size, shuffle=False\n",
    "# )\n",
    "\n",
    "# CFG.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/51 [00:00<?, ?batch/s]/home/lex/miniconda3/envs/airi/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 51/51 [00:14<00:00,  3.58batch/s]\n",
      "Epoch 0: 100%|██████████| 13/13 [00:01<00:00, 11.67batch/s]\n",
      "Epoch 1: 100%|██████████| 51/51 [00:14<00:00,  3.64batch/s]\n",
      "Epoch 1: 100%|██████████| 13/13 [00:01<00:00, 11.72batch/s]\n",
      "Epoch 2: 100%|██████████| 51/51 [00:13<00:00,  3.64batch/s]\n",
      "Epoch 2: 100%|██████████| 13/13 [00:01<00:00, 12.29batch/s]\n",
      "Epoch 3: 100%|██████████| 51/51 [00:13<00:00,  3.70batch/s]\n",
      "Epoch 3: 100%|██████████| 13/13 [00:01<00:00, 12.29batch/s]\n",
      "Epoch 4: 100%|██████████| 51/51 [00:11<00:00,  4.26batch/s]\n",
      "Epoch 4: 100%|██████████| 13/13 [00:00<00:00, 14.58batch/s]\n",
      "Epoch 5: 100%|██████████| 51/51 [00:11<00:00,  4.28batch/s]\n",
      "Epoch 5: 100%|██████████| 13/13 [00:00<00:00, 14.55batch/s]\n",
      "Epoch 6: 100%|██████████| 51/51 [00:12<00:00,  4.23batch/s]\n",
      "Epoch 6: 100%|██████████| 13/13 [00:00<00:00, 14.48batch/s]\n",
      "Epoch 7: 100%|██████████| 51/51 [00:12<00:00,  4.23batch/s]\n",
      "Epoch 7: 100%|██████████| 13/13 [00:00<00:00, 13.44batch/s]\n",
      "Epoch 8: 100%|██████████| 51/51 [00:12<00:00,  4.21batch/s]\n",
      "Epoch 8: 100%|██████████| 13/13 [00:01<00:00, 11.79batch/s]\n",
      "Epoch 9: 100%|██████████| 51/51 [00:13<00:00,  3.65batch/s]\n",
      "Epoch 9: 100%|██████████| 13/13 [00:01<00:00, 12.17batch/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "linear_schedule = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lambda i: min(1.0, i / (EPOCHS * len(train_data_loader)))\n",
    ")\n",
    "# tensorboard pytorch logging\n",
    "\n",
    "\n",
    "if True:\n",
    "    writer = SummaryWriter()\n",
    "    # training loop\n",
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "        for j, (X, y) in tqdm(\n",
    "            enumerate(train_data_loader),\n",
    "            unit=\"batch\",\n",
    "            total=len(train_data_loader),\n",
    "            desc=f\"Epoch {i}\",\n",
    "        ):\n",
    "            X = X.to(CFG.device)\n",
    "            y = y.to(CFG.device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(X, y)\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), time.time())\n",
    "            writer.add_scalar(\n",
    "                \"Learning rate\", optimizer.param_groups[0][\"lr\"], time.time()\n",
    "            )\n",
    "            writer.add_scalar(\"Epoch\", i, time.time())\n",
    "            loss.backward()\n",
    "            linear_schedule.step()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_correct = 0\n",
    "            for i, (X, y) in tqdm(\n",
    "                enumerate(test_data_loader),\n",
    "                unit=\"batch\",\n",
    "                total=len(test_data_loader),\n",
    "                desc=f\"Epoch {i}\",\n",
    "            ):\n",
    "                X = X.to(CFG.device)\n",
    "                y = y.to(CFG.device)\n",
    "                logits, loss = model(X, y)\n",
    "                total_correct += (logits.argmax(1) == y).sum().item()\n",
    "                writer.add_scalar(\"Loss/val\", loss, time.time())\n",
    "                writer.add_scalar(\"Epoch\", i, time.time())\n",
    "            writer.add_scalar(\n",
    "                \"Accuracy/val\", total_correct / len(test_data_loader), time.time()\n",
    "            )\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution with Claude help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADTokenizer:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.state_bins = np.linspace(cfg.state_min, cfg.state_max, cfg.state_bins)\n",
    "        self.reward_bins = np.linspace(cfg.reward_min, cfg.reward_max, cfg.reward_bins)\n",
    "        self.vocab_size = (\n",
    "            cfg.state_bins * 2 + cfg.action_dim + cfg.reward_bins + 3\n",
    "        )  # +3 for start, end, and pad tokens\n",
    "\n",
    "    def discretize(self, value, bins):\n",
    "        return np.digitize(value, bins)\n",
    "\n",
    "    def encode(self, x):\n",
    "        tokens = []\n",
    "        for state_x, state_y, action, reward in x:\n",
    "            tokens.extend(\n",
    "                [\n",
    "                    self.discretize(state_x, self.state_bins) + 3,\n",
    "                    self.discretize(state_y, self.state_bins) + self.cfg.state_bins + 3,\n",
    "                    action + self.cfg.state_bins * 2 + 3,\n",
    "                    self.discretize(reward, self.reward_bins)\n",
    "                    + self.cfg.state_bins * 2\n",
    "                    + self.cfg.action_dim\n",
    "                    + 3,\n",
    "                ]\n",
    "            )\n",
    "        return [self.cfg.start_token] + tokens + [self.cfg.end_token]\n",
    "\n",
    "    def pad(self, x):\n",
    "        return x + [self.cfg.padding_token] * (self.cfg.block_size - len(x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.pad(self.encode(x))\n",
    "\n",
    "\n",
    "# class A3CDataset(Dataset):\n",
    "#     def __init__(self, global_history, tokenizer, use_crop=True):\n",
    "#         self.global_history = global_history\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.use_crop = use_crop\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.global_history)\n",
    "\n",
    "#     def crop(self, arr):\n",
    "#         if len(arr) > self.tokenizer.cfg.block_size // 4:\n",
    "#             arr = arr[: self.tokenizer.cfg.block_size // 4]\n",
    "#         take_first = np.random.randint(2, len(arr))\n",
    "#         target = arr[-1]\n",
    "#         arr = arr[: take_first - 1]\n",
    "#         return arr, target\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         learning_history = self.global_history[idx]\n",
    "#         if self.use_crop:\n",
    "#             learning_history, target = self.crop(learning_history)\n",
    "#         learning_history = learning_history[: len(learning_history) - 1]\n",
    "#         # target = learning_history[-1]\n",
    "#         tokenized = self.tokenizer(learning_history)\n",
    "#         tensor = torch.tensor(tokenized)\n",
    "#         action = torch.tensor(target[2])\n",
    "#         return tensor, action\n",
    "\n",
    "\n",
    "class ADTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.token_embedding = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n",
    "        self.position_embedding = nn.Embedding(cfg.block_size, cfg.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(cfg.n_embd, cfg.n_head) for _ in range(cfg.n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(cfg.n_embd)\n",
    "        self.action_head = nn.Linear(cfg.n_embd, cfg.action_dim)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(t, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        action_logits = self.action_head(\n",
    "            x[:, -1, :]\n",
    "        )  # Only predict for the last position\n",
    "\n",
    "        # action_logits = self.action_head(x)  # Only predict for the last position\n",
    "\n",
    "        return action_logits\n",
    "\n",
    "    def predict_action(self, context):\n",
    "        action_logits = self(context)\n",
    "        return torch.argmax(action_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = ADTokenizer(CFG)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    A3CDataset(train_data, tokenizer=tokenizer),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    A3CDataset(test_data, tokenizer=tokenizer), batch_size=CFG.batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# CFG.vocab_size = tokenizer.vocab_size\n",
    "CFG.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = ADTransformer(CFG).to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 51/51 [00:14<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.7705, Val Loss: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 51/51 [00:14<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.3737, Val Loss: 0.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 51/51 [00:14<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2396, Val Loss: 0.3980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 51/51 [00:14<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.3297, Val Loss: 0.3920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 51/51 [00:13<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.4888, Val Loss: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 51/51 [00:14<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.3446, Val Loss: 0.3695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 51/51 [00:12<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.4363, Val Loss: 0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 51/51 [00:13<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.3426, Val Loss: 0.3986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 51/51 [00:13<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.4923, Val Loss: 0.3698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 51/51 [00:13<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.6643, Val Loss: 0.3628\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=EPOCHS * len(train_data_loader)\n",
    ")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for j, (X, y) in tqdm(\n",
    "        enumerate(train_data_loader),\n",
    "        total=len(train_data_loader),\n",
    "        desc=f\"Epoch {epoch}\",\n",
    "    ):\n",
    "        X, y = X.to(CFG.device), y.to(CFG.device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "\n",
    "        # B, T, C = logits.shape\n",
    "        # logits = logits.view(B, C * T)\n",
    "        # targets = targets.view(B * T)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # loss = F.cross_entropy(logits, y)\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_data_loader) + j)\n",
    "        writer.add_scalar(\n",
    "            \"Learning rate\",\n",
    "            scheduler.get_last_lr()[0],\n",
    "            epoch * len(train_data_loader) + j,\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_data_loader:\n",
    "            X, y = X.to(CFG.device), y.to(CFG.device)\n",
    "            logits = model(X)\n",
    "            # B, T, C = logits.shape\n",
    "            # logits = logits.view(B, C * T)\n",
    "            val_loss += F.cross_entropy(logits, y).item()\n",
    "    val_loss /= len(test_data_loader)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8137\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_data_loader:\n",
    "        X, y = X.to(CFG.device), y.to(CFG.device)\n",
    "        logits = model(X)\n",
    "        # B, T, C = logits.shape\n",
    "        # logits = logits.view(B, C * T)\n",
    "        predicted = torch.argmax(logits, dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_171193/2282299299.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dark_room import DarkRoom\n",
    "from time import sleep\n",
    "from utils import print_grid\n",
    "\n",
    "\n",
    "model = model.eval().cpu()\n",
    "env = DarkRoom(size=14, goal=(0, 0))\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "action_history = [[4.0, 4.0, 0, 0]]\n",
    "global_reward = 0\n",
    "\n",
    "\n",
    "def make_iteration(state, global_reward):\n",
    "    tokenized = torch.tensor([tokenizer(action_history)], dtype=torch.long)\n",
    "    logits = model(tokenized)\n",
    "    # B, T, C = logits.shape\n",
    "    # logits = logits.view(B, C * T)\n",
    "    action = torch.argmax(logits, dim=1).item()\n",
    "    #     action = torch.softmax(policy[0], dim=-1).argmax().item()\n",
    "    # action = Categorical(policy).sample().item()\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "    action_history.append([state[0], state[1], action, reward])\n",
    "\n",
    "    return state, global_reward + reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.0, 4.0, 0, 0]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached\n",
      "\u001b[H\u001b[J\n",
      "[['G' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "state, global_reward, done = make_iteration(state, global_reward)\n",
    "print_grid(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7790\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/51 [00:00<?, ?batch/s]../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "Epoch 0:   0%|          | 0/51 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[275], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(X, y)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoss/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m     26\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, i, time\u001b[38;5;241m.\u001b[39mtime())\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py:378\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add scalar data to summary.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mscalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/summary.py:371\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscalar\u001b[39m(name, tensor, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, new_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, double_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Output a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmake_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    373\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    374\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/_convert_np.py:24\u001b[0m, in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_prepare_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but numpy array or torch tensor are expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/_convert_np.py:33\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dark_room import DarkRoom\n",
    "from time import sleep\n",
    "\n",
    "from utils import print_grid\n",
    "\n",
    "model.eval()\n",
    "env = DarkRoom(size=16)\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "action_history = [[4.0, 4.0, 0, 0]]\n",
    "global_reward = 0\n",
    "\n",
    "\n",
    "def make_iteration(state):\n",
    "    tokenized = torch.tensor([tokenizer(action_history)], dtype=torch.long).to(\n",
    "        CFG.device\n",
    "    )\n",
    "    policy = model(tokenized)\n",
    "    action = torch.softmax(policy[0], dim=-1).argmax().item()\n",
    "    # action = Categorical(policy).sample().item()\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "    action_history.append([state[0], state[1], action, reward])\n",
    "\n",
    "    return state, reward, done\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     state, reward, done = make_iteration(state)\n",
    "#     sleep(0.1)\n",
    "#     if done:\n",
    "#         print(f\"Goal reached in {i} steps\")\n",
    "#         print(f\"Action history: {action_history}\")\n",
    "#         print_grid(env.render())\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.0, 4.0, 0, 0]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached\n",
      "\u001b[H\u001b[J\n",
      "[['G' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "state, reward, done = make_iteration(state)\n",
    "print_grid(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2], -1, False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
