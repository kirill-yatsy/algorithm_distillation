{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from itertools import chain\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 5\n"
     ]
    }
   ],
   "source": [
    "start_x = np.random.choice([0, 2])\n",
    "x = np.random.randint(start_x, 9)\n",
    "y = np.random.randint(0 if start_x == 2 else 2, 9)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed of all polssible random number generators\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# enable CUDA_LAUNCH_BLOCKING=1 to debug cuda\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_frame():\n",
    "    df = pd.read_csv(\"data/all_data.csv\")\n",
    "    episodes = df.groupby(\"Episode\")\n",
    "    episode_data = {}\n",
    "    for episode, data in episodes:\n",
    "        episode_data[episode] = [\n",
    "            [row[\"X\"], row[\"Y\"], row[\"Action\"], row[\"Reward\"]]\n",
    "            for index, row in data.iterrows()\n",
    "        ]\n",
    "    return episode_data\n",
    "\n",
    "\n",
    "episode_data = [episode for episode in read_data_frame().values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11865"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(episode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data and split it into training and testing\n",
    "np.random.shuffle(episode_data)\n",
    "train_data = episode_data[: int(len(episode_data) * 0.8)]\n",
    "test_data = episode_data[int(len(episode_data) * 0.8) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    block_size = 512\n",
    "    start_token = 0\n",
    "    max_grid_size = 50\n",
    "    padding_token = 1\n",
    "    end_token = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    state_min, state_max = 0, 9  # Adjust based on your environment\n",
    "    state_bins = 10\n",
    "    reward_min, reward_max = -1, 1  # Adjust based on your environment\n",
    "    reward_bins = 20\n",
    "    action_dim = 5\n",
    "    n_embd = 128\n",
    "    n_head = 2\n",
    "    n_layer = 3\n",
    "    dropout = 0.2\n",
    "    vocab_size = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    mapper = dict(\n",
    "        {\n",
    "            CFG.start_token: CFG.start_token,\n",
    "            CFG.padding_token: CFG.padding_token,\n",
    "            CFG.end_token: CFG.end_token,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    vocab_size = CFG.max_grid_size * 2 + CFG.action_dim + 2 + 3\n",
    "    offset = 3\n",
    "    coordinate_demention_size = 100  # max grid size\n",
    "    token_counter = 3\n",
    "\n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    # gets one unicode number. It should check if the unicode number is already in the mapper. If not, it should add it. Returns the number.\n",
    "    def get_tokenized_unicode(self, x):\n",
    "        if x not in self.mapper:\n",
    "            self.mapper[x] = self.token_counter\n",
    "            self.token_counter += 1\n",
    "        return self.mapper[x]\n",
    "\n",
    "    def encode(self, x: np.array):\n",
    "        x = np.array(x)\n",
    "        tokens = []\n",
    "        for i in range(0, len(x)):\n",
    "            step = np.array(x[i])\n",
    "            tokens.extend(\n",
    "                [\n",
    "                    step[0] + self.offset,\n",
    "                    step[1] + self.offset + CFG.max_grid_size,\n",
    "                    step[2] + self.offset + CFG.max_grid_size * 2,\n",
    "                    step[3] + self.offset + CFG.max_grid_size * 2 + CFG.action_dim,\n",
    "                ]\n",
    "            )\n",
    "        return [CFG.start_token] + tokens + [CFG.end_token]\n",
    "\n",
    "    def pad(self, x):\n",
    "        return x + [CFG.padding_token] * (CFG.block_size - len(x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        padded = self.pad(encoded)\n",
    "        if len(padded) != CFG.block_size:\n",
    "            print(\"padding error\")\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, global_history, tokenizer):\n",
    "        self.global_history = global_history\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.global_history)\n",
    "\n",
    "    def crop(self, arr):\n",
    "        if len(arr) > CFG.block_size // 4:\n",
    "            # make sequence CFG.block_size wize by randomly cropping the sequence\n",
    "            start_index = np.random.randint(0, len(arr) - CFG.block_size // 4)\n",
    "            arr = arr[start_index : start_index + CFG.block_size // 4]\n",
    "\n",
    "        take_first = np.random.randint(2, len(arr))\n",
    "        target = arr[-1]\n",
    "        arr = arr[: take_first - 1]\n",
    "        return arr, target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        learning_history = self.global_history[idx]\n",
    "\n",
    "        learning_history, target = self.crop(learning_history)\n",
    "\n",
    "        tokenized = self.tokenizer(learning_history)\n",
    "        tensor = torch.tensor(tokenized, dtype=torch.long)\n",
    "        action = torch.tensor(target[2], dtype=torch.long)\n",
    "        if tensor.shape[0] > CFG.block_size:\n",
    "            print(tensor.shape)\n",
    "\n",
    "        return tensor, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = A3CDataset(episode_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset, batch_size=CFG.batch_size)\n",
    "\n",
    "# for i, batch in enumerate(loader):\n",
    "#     print(i)\n",
    "#     print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3219"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3219"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A3CDataset(train_data, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    A3CDataset(train_data, tokenizer=tokenizer),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    A3CDataset(test_data, tokenizer=tokenizer), batch_size=CFG.batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [tensor([[ 0,  7, 56,  ...,  1,  1,  1],\n",
      "        [ 0,  6, 57,  ...,  1,  1,  1],\n",
      "        [ 0,  7, 56,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6, 57,  ...,  1,  1,  1],\n",
      "        [ 0,  7, 56,  ...,  1,  1,  1],\n",
      "        [ 0,  7, 56,  ...,  1,  1,  1]]), tensor([2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2,\n",
      "        2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0,\n",
      "        2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"sample: {next(iter(train_data_loader))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find vocabulary size base on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_loader = DataLoader(\n",
    "    A3CDataset(episode_data, tokenizer=tokenizer),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 0\n"
     ]
    }
   ],
   "source": [
    "for x, y in all_data_loader:\n",
    "    # noop\n",
    "    a = 2\n",
    "\n",
    "print(f\"Vocab size: {CFG.vocab_size}\")\n",
    "CFG.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660101 M parameters\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "# batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "# max_iters = 5000\n",
    "# eval_interval = 500\n",
    "# learning_rate = 3e-4\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# eval_iters = 200\n",
    "# n_embd = 384\n",
    "# n_head = 8\n",
    "# n_layer = 8\n",
    "# dropout = 0.2\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(CFG.n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(CFG.n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(CFG.n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\n",
    "            \"tril\", torch.tril(torch.ones(CFG.block_size, CFG.block_size))\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(CFG.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (\n",
    "            q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        )  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, CFG.n_embd)\n",
    "        self.dropout = nn.Dropout(CFG.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(CFG.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(CFG.dropout),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(CFG.vocab_size, CFG.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(CFG.block_size, CFG.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(CFG.n_embd, n_head=CFG.n_head) for _ in range(CFG.n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(CFG.n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(CFG.n_embd, CFG.action_dim)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        # tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=CFG.device)\n",
    "        )  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x[:, -1, :])  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # B, T, C = logits.shape\n",
    "            # logits = logits.view(B, C * T)\n",
    "            # targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(CFG.device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = ADTokenizer(CFG)\n",
    "\n",
    "# train_data_loader = DataLoader(\n",
    "#     A3CDataset(train_data, tokenizer=tokenizer),\n",
    "#     batch_size=CFG.batch_size,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "# test_data_loader = DataLoader(\n",
    "#     A3CDataset(test_data, tokenizer=tokenizer), batch_size=CFG.batch_size, shuffle=False\n",
    "# )\n",
    "\n",
    "# CFG.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/51 [00:00<?, ?batch/s]/home/lex/miniconda3/envs/airi/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 51/51 [00:02<00:00, 21.97batch/s]\n",
      "Epoch 0: 100%|██████████| 13/13 [00:00<00:00, 80.18batch/s]\n",
      "Epoch 1: 100%|██████████| 51/51 [00:02<00:00, 24.88batch/s]\n",
      "Epoch 1: 100%|██████████| 13/13 [00:00<00:00, 84.22batch/s]\n",
      "Epoch 2: 100%|██████████| 51/51 [00:01<00:00, 26.40batch/s]\n",
      "Epoch 2: 100%|██████████| 13/13 [00:00<00:00, 64.70batch/s]\n",
      "Epoch 3: 100%|██████████| 51/51 [00:01<00:00, 26.17batch/s]\n",
      "Epoch 3: 100%|██████████| 13/13 [00:00<00:00, 62.55batch/s]\n",
      "Epoch 4: 100%|██████████| 51/51 [00:01<00:00, 25.85batch/s]\n",
      "Epoch 4: 100%|██████████| 13/13 [00:00<00:00, 63.13batch/s]\n",
      "Epoch 5: 100%|██████████| 51/51 [00:01<00:00, 25.79batch/s]\n",
      "Epoch 5: 100%|██████████| 13/13 [00:00<00:00, 61.44batch/s]\n",
      "Epoch 6: 100%|██████████| 51/51 [00:01<00:00, 25.77batch/s]\n",
      "Epoch 6: 100%|██████████| 13/13 [00:00<00:00, 81.56batch/s]\n",
      "Epoch 7: 100%|██████████| 51/51 [00:01<00:00, 25.97batch/s]\n",
      "Epoch 7: 100%|██████████| 13/13 [00:00<00:00, 86.33batch/s]\n",
      "Epoch 8: 100%|██████████| 51/51 [00:01<00:00, 26.95batch/s]\n",
      "Epoch 8: 100%|██████████| 13/13 [00:00<00:00, 71.58batch/s]\n",
      "Epoch 9: 100%|██████████| 51/51 [00:01<00:00, 27.24batch/s]\n",
      "Epoch 9: 100%|██████████| 13/13 [00:00<00:00, 86.07batch/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "linear_schedule = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lambda i: min(1.0, i / (EPOCHS * len(train_data_loader)))\n",
    ")\n",
    "# tensorboard pytorch logging\n",
    "\n",
    "\n",
    "if True:\n",
    "    writer = SummaryWriter()\n",
    "    # training loop\n",
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "        for j, (X, y) in tqdm(\n",
    "            enumerate(train_data_loader),\n",
    "            unit=\"batch\",\n",
    "            total=len(train_data_loader),\n",
    "            desc=f\"Epoch {i}\",\n",
    "        ):\n",
    "            X = X.to(CFG.device)\n",
    "            y = y.to(CFG.device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(X, y)\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), time.time())\n",
    "            writer.add_scalar(\n",
    "                \"Learning rate\", optimizer.param_groups[0][\"lr\"], time.time()\n",
    "            )\n",
    "            writer.add_scalar(\"Epoch\", i, time.time())\n",
    "            loss.backward()\n",
    "            linear_schedule.step()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_correct = 0\n",
    "            for i, (X, y) in tqdm(\n",
    "                enumerate(test_data_loader),\n",
    "                unit=\"batch\",\n",
    "                total=len(test_data_loader),\n",
    "                desc=f\"Epoch {i}\",\n",
    "            ):\n",
    "                X = X.to(CFG.device)\n",
    "                y = y.to(CFG.device)\n",
    "                logits, loss = model(X, y)\n",
    "                total_correct += (logits.argmax(1) == y).sum().item()\n",
    "                writer.add_scalar(\"Loss/val\", loss, time.time())\n",
    "                writer.add_scalar(\"Epoch\", i, time.time())\n",
    "            writer.add_scalar(\n",
    "                \"Accuracy/val\", total_correct / len(test_data_loader), time.time()\n",
    "            )\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution with Claude help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADTokenizer:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.state_bins = np.linspace(cfg.state_min, cfg.state_max, cfg.state_bins)\n",
    "        self.reward_bins = np.linspace(cfg.reward_min, cfg.reward_max, cfg.reward_bins)\n",
    "        self.vocab_size = (\n",
    "            cfg.state_bins * 2 + cfg.action_dim + cfg.reward_bins + 3\n",
    "        )  # +3 for start, end, and pad tokens\n",
    "\n",
    "    def discretize(self, value, bins):\n",
    "        return np.digitize(value, bins)\n",
    "\n",
    "    def encode(self, x):\n",
    "        tokens = []\n",
    "        for state_x, state_y, action, reward in x:\n",
    "            tokens.extend(\n",
    "                [\n",
    "                    self.discretize(state_x, self.state_bins) + 3,\n",
    "                    self.discretize(state_y, self.state_bins) + self.cfg.state_bins + 3,\n",
    "                    action + self.cfg.state_bins * 2 + 3,\n",
    "                    self.discretize(reward, self.reward_bins)\n",
    "                    + self.cfg.state_bins * 2\n",
    "                    + self.cfg.action_dim\n",
    "                    + 3,\n",
    "                ]\n",
    "            )\n",
    "        return [self.cfg.start_token] + tokens + [self.cfg.end_token]\n",
    "\n",
    "    def pad(self, x):\n",
    "        return x + [self.cfg.padding_token] * (self.cfg.block_size - len(x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.pad(self.encode(x))\n",
    "\n",
    "\n",
    "# class A3CDataset(Dataset):\n",
    "#     def __init__(self, global_history, tokenizer, use_crop=True):\n",
    "#         self.global_history = global_history\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.use_crop = use_crop\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.global_history)\n",
    "\n",
    "#     def crop(self, arr):\n",
    "#         if len(arr) > self.tokenizer.cfg.block_size // 4:\n",
    "#             arr = arr[: self.tokenizer.cfg.block_size // 4]\n",
    "#         take_first = np.random.randint(2, len(arr))\n",
    "#         target = arr[-1]\n",
    "#         arr = arr[: take_first - 1]\n",
    "#         return arr, target\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         learning_history = self.global_history[idx]\n",
    "#         if self.use_crop:\n",
    "#             learning_history, target = self.crop(learning_history)\n",
    "#         learning_history = learning_history[: len(learning_history) - 1]\n",
    "#         # target = learning_history[-1]\n",
    "#         tokenized = self.tokenizer(learning_history)\n",
    "#         tensor = torch.tensor(tokenized)\n",
    "#         action = torch.tensor(target[2])\n",
    "#         return tensor, action\n",
    "\n",
    "\n",
    "class ADTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.token_embedding = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n",
    "        self.position_embedding = nn.Embedding(cfg.block_size, cfg.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(cfg.n_embd, cfg.n_head) for _ in range(cfg.n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(cfg.n_embd)\n",
    "        self.action_head = nn.Linear(cfg.n_embd, cfg.action_dim)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(t, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        action_logits = self.action_head(\n",
    "            x[:, -1, :]\n",
    "        )  # Only predict for the last position\n",
    "\n",
    "        # action_logits = self.action_head(x)  # Only predict for the last position\n",
    "\n",
    "        return action_logits\n",
    "\n",
    "    def predict_action(self, context):\n",
    "        action_logits = self(context)\n",
    "        return torch.argmax(action_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = ADTokenizer(CFG)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    A3CDataset(train_data, tokenizer=tokenizer),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    A3CDataset(test_data, tokenizer=tokenizer), batch_size=CFG.batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# CFG.vocab_size = tokenizer.vocab_size\n",
    "CFG.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = ADTransformer(CFG).to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 165/165 [00:06<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.6232, Val Loss: 0.6634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 165/165 [00:06<00:00, 26.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.8385, Val Loss: 0.6242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 165/165 [00:06<00:00, 26.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.3502, Val Loss: 0.6284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 165/165 [00:06<00:00, 26.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.6057, Val Loss: 0.6210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 165/165 [00:06<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.7102, Val Loss: 0.6201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 165/165 [00:06<00:00, 25.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.5489, Val Loss: 0.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 165/165 [00:06<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.7933, Val Loss: 0.6145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 165/165 [00:06<00:00, 26.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.4014, Val Loss: 0.6075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 165/165 [00:06<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.5569, Val Loss: 0.6022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 165/165 [00:06<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.3818, Val Loss: 0.6095\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=EPOCHS * len(train_data_loader)\n",
    ")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for j, (X, y) in tqdm(\n",
    "        enumerate(train_data_loader),\n",
    "        total=len(train_data_loader),\n",
    "        desc=f\"Epoch {epoch}\",\n",
    "    ):\n",
    "        X, y = X.to(CFG.device), y.to(CFG.device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "\n",
    "        # B, T, C = logits.shape\n",
    "        # logits = logits.view(B, C * T)\n",
    "        # targets = targets.view(B * T)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # loss = F.cross_entropy(logits, y)\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_data_loader) + j)\n",
    "        writer.add_scalar(\n",
    "            \"Learning rate\",\n",
    "            scheduler.get_last_lr()[0],\n",
    "            epoch * len(train_data_loader) + j,\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_data_loader:\n",
    "            X, y = X.to(CFG.device), y.to(CFG.device)\n",
    "            logits = model(X)\n",
    "            # B, T, C = logits.shape\n",
    "            # logits = logits.view(B, C * T)\n",
    "            val_loss += F.cross_entropy(logits, y).item()\n",
    "    val_loss /= len(test_data_loader)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6650\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_data_loader:\n",
    "        X, y = X.to(CFG.device), y.to(CFG.device)\n",
    "        logits = model(X)\n",
    "        # B, T, C = logits.shape\n",
    "        # logits = logits.view(B, C * T)\n",
    "        predicted = torch.argmax(logits, dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "# model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from dark_room import DarkRoom\n",
    "\n",
    "env = DarkRoom()\n",
    "\n",
    "\n",
    "def gpt_step(action_history, max_len=CFG.block_size // 8 - 1):\n",
    "    if len(action_history) > max_len:\n",
    "        action_history = action_history[-max_len:]\n",
    "    tokenized = torch.tensor(\n",
    "        [tokenizer(action_history)], dtype=torch.long, device=\"cpu\"\n",
    "    )\n",
    "    logits = model(tokenized)\n",
    "    action = torch.argmax(logits, dim=1).item()\n",
    "    return action\n",
    "\n",
    "\n",
    "def render_env(env, title, speepTime=0):\n",
    "    # clear previous image\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "    plt.title(title)\n",
    "    # plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    time.sleep(speepTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "history = [[state[0], state[1], 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "action = gpt_step(history)\n",
    "state, reward, done = env.step(action)\n",
    "history.append([state[0], state[1], action, reward])\n",
    "render_env(env, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     total_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     24\u001b[0m     steps_per_episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mrender_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m mapper[episode] \u001b[38;5;241m=\u001b[39m steps_per_episode\n\u001b[1;32m     27\u001b[0m returns\u001b[38;5;241m.\u001b[39mappend(total_return)\n",
      "Cell \u001b[0;32mIn[30], line 26\u001b[0m, in \u001b[0;36mrender_env\u001b[0;34m(env, title, speepTime)\u001b[0m\n\u001b[1;32m     24\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(title)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# plt.axis(\"off\")\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(speepTime)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/pyplot.py:612\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/IPython/core/formatters.py:182\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    180\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/IPython/core/formatters.py:226\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    345\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/IPython/core/pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 170\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/backend_bases.py:2178\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2178\u001b[0m         bbox_inches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m                 pad_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2182\u001b[0m             h_pad \u001b[38;5;241m=\u001b[39m layout_engine\u001b[38;5;241m.\u001b[39mget()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh_pad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/_api/deprecation.py:457\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    452\u001b[0m     warn_deprecated(\n\u001b[1;32m    453\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    456\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/figure.py:1787\u001b[0m, in \u001b[0;36mFigureBase.get_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax\u001b[38;5;241m.\u001b[39mget_visible():\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;66;03m# some Axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1787\u001b[0m         bbox \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1790\u001b[0m         bbox \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mget_tightbbox(renderer)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/_api/deprecation.py:457\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    452\u001b[0m     warn_deprecated(\n\u001b[1;32m    453\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    456\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/axes/_base.py:4482\u001b[0m, in \u001b[0;36m_AxesBase.get_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4480\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_axis_map\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   4481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxison \u001b[38;5;129;01mand\u001b[39;00m axis\u001b[38;5;241m.\u001b[39mget_visible():\n\u001b[0;32m-> 4482\u001b[0m         ba \u001b[38;5;241m=\u001b[39m \u001b[43mmartist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tightbbox_for_layout_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4483\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ba:\n\u001b[1;32m   4484\u001b[0m             bb\u001b[38;5;241m.\u001b[39mappend(ba)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/artist.py:1408\u001b[0m, in \u001b[0;36m_get_tightbbox_for_layout_only\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;124;03mMatplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;124;03m*for_layout_only* kwarg; this helper tries to use the kwarg but skips it\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;124;03mwhen encountering third-party subclasses that do not support it.\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfor_layout_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mget_tightbbox(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/axis.py:1371\u001b[0m, in \u001b[0;36mAxis.get_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m   1369\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/axis.py:2645\u001b[0m, in \u001b[0;36mYAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;66;03m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;66;03m# that have been set by `fig.align_ylabels()`\u001b[39;00m\n\u001b[0;32m-> 2645\u001b[0m bboxes, bboxes2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick_boxes_siblings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2646\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mget_position()\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/axis.py:2197\u001b[0m, in \u001b[0;36mAxis._get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2195\u001b[0m axis \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_axis_map[name]\n\u001b[1;32m   2196\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 2197\u001b[0m tlb, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ticklabel_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks_to_draw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2198\u001b[0m bboxes\u001b[38;5;241m.\u001b[39mextend(tlb)\n\u001b[1;32m   2199\u001b[0m bboxes2\u001b[38;5;241m.\u001b[39mextend(tlb2)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/axis.py:1350\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1349\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1352\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1353\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/text.py:962\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    960\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n\u001b[1;32m    961\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\u001b[38;5;241m.\u001b[39mtransform((x, y))\n\u001b[0;32m--> 962\u001b[0m bbox \u001b[38;5;241m=\u001b[39m \u001b[43mbbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bbox\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/matplotlib/transforms.py:626\u001b[0m, in \u001b[0;36mBboxBase.translated\u001b[0;34m(self, tx, ty)\u001b[0m\n\u001b[1;32m    623\u001b[0m         h_pad \u001b[38;5;241m=\u001b[39m w_pad\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Bbox(points \u001b[38;5;241m+\u001b[39m [[\u001b[38;5;241m-\u001b[39mw_pad, \u001b[38;5;241m-\u001b[39mh_pad], [w_pad, h_pad]])\n\u001b[0;32m--> 626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslated\u001b[39m(\u001b[38;5;28mself\u001b[39m, tx, ty):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct a `Bbox` by translating this one by *tx* and *ty*.\"\"\"\u001b[39;00m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Bbox(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_points \u001b[38;5;241m+\u001b[39m (tx, ty))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "context = []\n",
    "returns = []\n",
    "num_episodes = 100\n",
    "mapper = {}\n",
    "\n",
    "env = DarkRoom(size=15)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset(use_random_agent_pos=False, agent_pos=[12, 8])\n",
    "    if len(context) == 0:\n",
    "        context = [[state[0], state[1], 0, 0]]\n",
    "    done = False\n",
    "    total_return = 0\n",
    "    steps_per_episode = 0\n",
    "    while not done:\n",
    "        action = gpt_step(context)\n",
    "        state, reward, done = env.step(action)\n",
    "        context.append([state[0], state[1], action, reward])\n",
    "        state, reward, done = env.step(action)\n",
    "        total_return += reward\n",
    "        steps_per_episode += 1\n",
    "        render_env(env, episode)\n",
    "    mapper[episode] = steps_per_episode\n",
    "    returns.append(total_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 10,\n",
       " 1: 14,\n",
       " 2: 41,\n",
       " 3: 17,\n",
       " 4: 16,\n",
       " 5: 55,\n",
       " 6: 17,\n",
       " 7: 52,\n",
       " 8: 17,\n",
       " 9: 31,\n",
       " 10: 26,\n",
       " 11: 18,\n",
       " 12: 55}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dark_room import DarkRoom\n",
    "from time import sleep\n",
    "from utils import print_grid\n",
    "\n",
    "\n",
    "model = model.eval().cpu()\n",
    "env = DarkRoom(size=14, goal=(0, 0))\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "action_history = [[4.0, 4.0, 0, 0]]\n",
    "global_reward = 0\n",
    "\n",
    "\n",
    "def make_iteration(state, global_reward):\n",
    "    tokenized = torch.tensor([tokenizer(action_history)], dtype=torch.long)\n",
    "    logits = model(tokenized)\n",
    "    # B, T, C = logits.shape\n",
    "    # logits = logits.view(B, C * T)\n",
    "    action = torch.argmax(logits, dim=1).item()\n",
    "    #     action = torch.softmax(policy[0], dim=-1).argmax().item()\n",
    "    # action = Categorical(policy).sample().item()\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "    action_history.append([state[0], state[1], action, reward])\n",
    "\n",
    "    return state, global_reward + reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.0, 4.0, 0, 0]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached\n",
      "\u001b[H\u001b[J\n",
      "[['G' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "state, global_reward, done = make_iteration(state, global_reward)\n",
    "print_grid(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7790\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/51 [00:00<?, ?batch/s]../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [526,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [616,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [96,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "Epoch 0:   0%|          | 0/51 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[275], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(X, y)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoss/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m     26\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, i, time\u001b[38;5;241m.\u001b[39mtime())\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py:378\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add scalar data to summary.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mscalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/summary.py:371\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscalar\u001b[39m(name, tensor, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, new_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, double_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Output a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmake_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    373\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    374\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/_convert_np.py:24\u001b[0m, in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_prepare_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but numpy array or torch tensor are expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/_convert_np.py:33\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dark_room import DarkRoom\n",
    "from time import sleep\n",
    "\n",
    "from utils import print_grid\n",
    "\n",
    "model.eval()\n",
    "env = DarkRoom(size=16)\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "action_history = [[4.0, 4.0, 0, 0]]\n",
    "global_reward = 0\n",
    "\n",
    "\n",
    "def make_iteration(state):\n",
    "    tokenized = torch.tensor([tokenizer(action_history)], dtype=torch.long).to(\n",
    "        CFG.device\n",
    "    )\n",
    "    policy = model(tokenized)\n",
    "    action = torch.softmax(policy[0], dim=-1).argmax().item()\n",
    "    # action = Categorical(policy).sample().item()\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "    action_history.append([state[0], state[1], action, reward])\n",
    "\n",
    "    return state, reward, done\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     state, reward, done = make_iteration(state)\n",
    "#     sleep(0.1)\n",
    "#     if done:\n",
    "#         print(f\"Goal reached in {i} steps\")\n",
    "#         print(f\"Action history: {action_history}\")\n",
    "#         print_grid(env.render())\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.0, 4.0, 0, 0]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached\n",
      "\u001b[H\u001b[J\n",
      "[['G' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "state, reward, done = make_iteration(state)\n",
    "print_grid(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2], -1, False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
