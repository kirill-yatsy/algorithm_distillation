{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from itertools import chain\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed of all polssible random number generators\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_frame():\n",
    "    df = pd.read_csv(\"all_data.csv\")\n",
    "    episodes = df.groupby(\"Episode\")\n",
    "    episode_data = {}\n",
    "    for episode, data in episodes:\n",
    "        episode_data[episode] = [\n",
    "            [row[\"X\"], row[\"Y\"], row[\"Action\"], row[\"Reward\"]]\n",
    "            for index, row in data.iterrows()\n",
    "        ]\n",
    "    return episode_data\n",
    "\n",
    "\n",
    "episode_data = [episode for episode in read_data_frame().values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data and split it into training and testing\n",
    "np.random.shuffle(episode_data)\n",
    "train_data = episode_data[: int(len(episode_data) * 0.8)]\n",
    "test_data = episode_data[int(len(episode_data) * 0.8) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    block_size = 512\n",
    "    start_token = 0\n",
    "    padding_token = 1\n",
    "    end_token = 3\n",
    "\n",
    "    batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    mapper = dict(\n",
    "        {\n",
    "            CFG.start_token: CFG.start_token,\n",
    "            CFG.padding_token: CFG.padding_token,\n",
    "            CFG.end_token: CFG.end_token,\n",
    "        }\n",
    "    )\n",
    "    token_counter = 3\n",
    "\n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    # gets one unicode number. It should check if the unicode number is already in the mapper. If not, it should add it. Returns the number.\n",
    "    def get_tokenized_unicode(self, x):\n",
    "        if x not in self.mapper:\n",
    "            self.mapper[x] = self.token_counter\n",
    "            self.token_counter += 1\n",
    "        return self.mapper[x]\n",
    "\n",
    "    def encode(self, x: np.array):\n",
    "        x = np.array(x)\n",
    "        tokens = []\n",
    "        for i in range(0, len(x)):\n",
    "            step = np.array(x[i])\n",
    "            step = \",\".join(step.astype(str))\n",
    "            for i in range(len(step)):\n",
    "                unicode_numb = ord(step[i])\n",
    "                tokens.append(self.get_tokenized_unicode(unicode_numb))\n",
    "        return [self.mapper[CFG.start_token]] + tokens + [self.mapper[CFG.end_token]]\n",
    "\n",
    "    def cut_to_max_len(self, x):\n",
    "        tokenized_steps = []\n",
    "        length = 0\n",
    "        for i in range(0, len(x)):\n",
    "            step = x[i]\n",
    "            tokenized_step = self.encode([step])\n",
    "            if length + len(tokenized_step) > CFG.block_size - 2:\n",
    "                return x[:i]\n",
    "            tokenized_steps.append(tokenized_step)\n",
    "            length += len(tokenized_step)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def pad(self, x):\n",
    "        return x + [CFG.padding_token] * (CFG.block_size - len(x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.pad(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, global_history, tokenizer, use_crop=True):\n",
    "        self.global_history = global_history\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_crop = use_crop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.global_history)\n",
    "\n",
    "    def crop(self, arr):\n",
    "        if len(arr) > CFG.block_size:\n",
    "            arr = arr[: CFG.block_size]\n",
    "\n",
    "        take_first = np.random.randint(2, len(arr))\n",
    "        target = arr[-1]\n",
    "        arr = arr[: take_first - 1]\n",
    "        return arr, target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        learning_history = self.global_history[idx]\n",
    "\n",
    "        if self.use_crop:\n",
    "            learning_history = self.tokenizer.cut_to_max_len(learning_history)\n",
    "            learning_history, target = self.crop(learning_history)\n",
    "\n",
    "        tokenized = self.tokenizer(learning_history)\n",
    "        tensor = torch.tensor(np.array(tokenized))\n",
    "        action = torch.tensor(target[2])\n",
    "        return tensor, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4830"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4830"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A3CDataset(train_data, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    A3CDataset(train_data, tokenizer=tokenizer),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    A3CDataset(test_data, tokenizer=tokenizer), batch_size=CFG.batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [tensor([[ 0,  3,  4,  ...,  1,  1,  1],\n",
      "        [ 0, 11,  4,  ...,  1,  1,  1],\n",
      "        [ 0,  3,  4,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  3,  4,  ...,  1,  1,  1],\n",
      "        [ 0,  3,  4,  ...,  1,  1,  1],\n",
      "        [ 0, 11,  4,  ...,  1,  1,  1]]), tensor([2, 4, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 0,\n",
      "        2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
      "        0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"sample: {next(iter(train_data_loader))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 3: 3,\n",
       " 52: 3,\n",
       " 46: 4,\n",
       " 48: 5,\n",
       " 44: 6,\n",
       " 51: 7,\n",
       " 45: 8,\n",
       " 49: 9,\n",
       " 50: 10,\n",
       " 53: 11,\n",
       " 54: 12}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find vocabulary size base on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 15\n"
     ]
    }
   ],
   "source": [
    "# find vocab size by iterating over the dataset\n",
    "all_data_loader = DataLoader(\n",
    "    A3CDataset(episode_data, tokenizer=tokenizer, use_crop=False),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for x, y in train_data_loader:\n",
    "    # noop\n",
    "    a = 2\n",
    "\n",
    "vocab_size = len(tokenizer.mapper)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.391557 M parameters\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "actions_demention = 5\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\n",
    "            \"tril\", torch.tril(torch.ones(CFG.block_size, CFG.block_size))\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (\n",
    "            q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        )  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(CFG.block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, actions_demention)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        try:\n",
    "            tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        except:\n",
    "            print(idx)\n",
    "            print(idx.shape)\n",
    "            print(targets)\n",
    "            print(targets.shape)\n",
    "            print(idx.shape)\n",
    "        # tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B, C * T)\n",
    "            # targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/76 [00:00<?, ?batch/s]/home/lex/miniconda3/envs/airi/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 76/76 [00:26<00:00,  2.82batch/s]\n",
      "Epoch 0: 100%|██████████| 19/19 [00:02<00:00,  8.69batch/s]\n",
      "Epoch 1: 100%|██████████| 76/76 [00:28<00:00,  2.71batch/s]\n",
      "Epoch 1: 100%|██████████| 19/19 [00:02<00:00,  8.14batch/s]\n",
      "Epoch 2: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 2: 100%|██████████| 19/19 [00:02<00:00,  7.89batch/s]\n",
      "Epoch 3: 100%|██████████| 76/76 [00:28<00:00,  2.66batch/s]\n",
      "Epoch 3: 100%|██████████| 19/19 [00:02<00:00,  7.57batch/s]\n",
      "Epoch 4: 100%|██████████| 76/76 [00:28<00:00,  2.63batch/s]\n",
      "Epoch 4: 100%|██████████| 19/19 [00:02<00:00,  8.55batch/s]\n",
      "Epoch 5: 100%|██████████| 76/76 [00:28<00:00,  2.71batch/s]\n",
      "Epoch 5: 100%|██████████| 19/19 [00:02<00:00,  8.50batch/s]\n",
      "Epoch 6: 100%|██████████| 76/76 [00:27<00:00,  2.78batch/s]\n",
      "Epoch 6: 100%|██████████| 19/19 [00:02<00:00,  8.47batch/s]\n",
      "Epoch 7: 100%|██████████| 76/76 [00:27<00:00,  2.79batch/s]\n",
      "Epoch 7: 100%|██████████| 19/19 [00:02<00:00,  7.77batch/s]\n",
      "Epoch 8: 100%|██████████| 76/76 [00:28<00:00,  2.70batch/s]\n",
      "Epoch 8: 100%|██████████| 19/19 [00:02<00:00,  8.54batch/s]\n",
      "Epoch 9: 100%|██████████| 76/76 [00:27<00:00,  2.80batch/s]\n",
      "Epoch 9: 100%|██████████| 19/19 [00:02<00:00,  8.54batch/s]\n",
      "Epoch 10: 100%|██████████| 76/76 [00:30<00:00,  2.53batch/s]\n",
      "Epoch 10: 100%|██████████| 19/19 [00:02<00:00,  7.55batch/s]\n",
      "Epoch 11: 100%|██████████| 76/76 [00:28<00:00,  2.64batch/s]\n",
      "Epoch 11: 100%|██████████| 19/19 [00:02<00:00,  8.16batch/s]\n",
      "Epoch 12: 100%|██████████| 76/76 [00:28<00:00,  2.66batch/s]\n",
      "Epoch 12: 100%|██████████| 19/19 [00:02<00:00,  8.25batch/s]\n",
      "Epoch 13: 100%|██████████| 76/76 [00:27<00:00,  2.71batch/s]\n",
      "Epoch 13: 100%|██████████| 19/19 [00:02<00:00,  7.81batch/s]\n",
      "Epoch 14: 100%|██████████| 76/76 [00:29<00:00,  2.58batch/s]\n",
      "Epoch 14: 100%|██████████| 19/19 [00:02<00:00,  7.89batch/s]\n",
      "Epoch 15: 100%|██████████| 76/76 [00:28<00:00,  2.70batch/s]\n",
      "Epoch 15: 100%|██████████| 19/19 [00:02<00:00,  8.25batch/s]\n",
      "Epoch 16: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 16: 100%|██████████| 19/19 [00:02<00:00,  8.29batch/s]\n",
      "Epoch 17: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 17: 100%|██████████| 19/19 [00:02<00:00,  8.21batch/s]\n",
      "Epoch 18: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 18: 100%|██████████| 19/19 [00:02<00:00,  8.18batch/s]\n",
      "Epoch 19: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 19: 100%|██████████| 19/19 [00:02<00:00,  8.09batch/s]\n",
      "Epoch 20: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 20: 100%|██████████| 19/19 [00:02<00:00,  8.30batch/s]\n",
      "Epoch 21: 100%|██████████| 76/76 [00:27<00:00,  2.75batch/s]\n",
      "Epoch 21: 100%|██████████| 19/19 [00:02<00:00,  8.25batch/s]\n",
      "Epoch 22: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 22: 100%|██████████| 19/19 [00:02<00:00,  8.25batch/s]\n",
      "Epoch 23: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 23: 100%|██████████| 19/19 [00:02<00:00,  8.29batch/s]\n",
      "Epoch 24: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 24: 100%|██████████| 19/19 [00:02<00:00,  8.34batch/s]\n",
      "Epoch 25: 100%|██████████| 76/76 [00:27<00:00,  2.75batch/s]\n",
      "Epoch 25: 100%|██████████| 19/19 [00:02<00:00,  8.10batch/s]\n",
      "Epoch 26: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 26: 100%|██████████| 19/19 [00:02<00:00,  8.35batch/s]\n",
      "Epoch 27: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 27: 100%|██████████| 19/19 [00:02<00:00,  8.39batch/s]\n",
      "Epoch 28: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 28: 100%|██████████| 19/19 [00:02<00:00,  8.17batch/s]\n",
      "Epoch 29: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 29: 100%|██████████| 19/19 [00:02<00:00,  8.36batch/s]\n",
      "Epoch 30: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 30: 100%|██████████| 19/19 [00:02<00:00,  8.24batch/s]\n",
      "Epoch 31: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 31: 100%|██████████| 19/19 [00:02<00:00,  8.14batch/s]\n",
      "Epoch 32: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 32: 100%|██████████| 19/19 [00:02<00:00,  8.33batch/s]\n",
      "Epoch 33: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 33: 100%|██████████| 19/19 [00:02<00:00,  8.34batch/s]\n",
      "Epoch 34: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 34: 100%|██████████| 19/19 [00:02<00:00,  8.19batch/s]\n",
      "Epoch 35: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 35: 100%|██████████| 19/19 [00:02<00:00,  8.38batch/s]\n",
      "Epoch 36: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 36: 100%|██████████| 19/19 [00:02<00:00,  8.28batch/s]\n",
      "Epoch 37: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 37: 100%|██████████| 19/19 [00:02<00:00,  8.25batch/s]\n",
      "Epoch 38: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 38: 100%|██████████| 19/19 [00:02<00:00,  8.27batch/s]\n",
      "Epoch 39: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 39: 100%|██████████| 19/19 [00:02<00:00,  8.32batch/s]\n",
      "Epoch 40: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 40: 100%|██████████| 19/19 [00:02<00:00,  8.33batch/s]\n",
      "Epoch 41: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 41: 100%|██████████| 19/19 [00:02<00:00,  8.32batch/s]\n",
      "Epoch 42: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 42: 100%|██████████| 19/19 [00:02<00:00,  8.40batch/s]\n",
      "Epoch 43: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 43: 100%|██████████| 19/19 [00:02<00:00,  8.35batch/s]\n",
      "Epoch 44: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 44: 100%|██████████| 19/19 [00:02<00:00,  8.27batch/s]\n",
      "Epoch 45: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 45: 100%|██████████| 19/19 [00:02<00:00,  8.20batch/s]\n",
      "Epoch 46: 100%|██████████| 76/76 [00:27<00:00,  2.75batch/s]\n",
      "Epoch 46: 100%|██████████| 19/19 [00:02<00:00,  8.27batch/s]\n",
      "Epoch 47: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 47: 100%|██████████| 19/19 [00:02<00:00,  8.14batch/s]\n",
      "Epoch 48: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 48: 100%|██████████| 19/19 [00:02<00:00,  8.17batch/s]\n",
      "Epoch 49: 100%|██████████| 76/76 [00:27<00:00,  2.75batch/s]\n",
      "Epoch 49: 100%|██████████| 19/19 [00:02<00:00,  8.26batch/s]\n",
      "Epoch 50: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 50: 100%|██████████| 19/19 [00:02<00:00,  8.24batch/s]\n",
      "Epoch 51: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 51: 100%|██████████| 19/19 [00:02<00:00,  8.25batch/s]\n",
      "Epoch 52: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 52: 100%|██████████| 19/19 [00:02<00:00,  8.34batch/s]\n",
      "Epoch 53: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 53: 100%|██████████| 19/19 [00:02<00:00,  8.32batch/s]\n",
      "Epoch 54: 100%|██████████| 76/76 [00:27<00:00,  2.73batch/s]\n",
      "Epoch 54: 100%|██████████| 19/19 [00:02<00:00,  8.36batch/s]\n",
      "Epoch 55: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 55: 100%|██████████| 19/19 [00:02<00:00,  8.27batch/s]\n",
      "Epoch 56: 100%|██████████| 76/76 [00:28<00:00,  2.71batch/s]\n",
      "Epoch 56: 100%|██████████| 19/19 [00:02<00:00,  8.30batch/s]\n",
      "Epoch 57: 100%|██████████| 76/76 [00:27<00:00,  2.72batch/s]\n",
      "Epoch 57: 100%|██████████| 19/19 [00:02<00:00,  8.18batch/s]\n",
      "Epoch 58: 100%|██████████| 76/76 [00:27<00:00,  2.74batch/s]\n",
      "Epoch 58: 100%|██████████| 19/19 [00:02<00:00,  8.09batch/s]\n",
      "Epoch 59: 100%|██████████| 76/76 [00:27<00:00,  2.75batch/s]\n",
      "Epoch 59: 100%|██████████| 19/19 [00:02<00:00,  8.75batch/s]\n",
      "Epoch 60: 100%|██████████| 76/76 [00:26<00:00,  2.90batch/s]\n",
      "Epoch 60: 100%|██████████| 19/19 [00:02<00:00,  8.68batch/s]\n",
      "Epoch 61:  83%|████████▎ | 63/76 [00:23<00:04,  2.71batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     41\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(X, y)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoss/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m     44\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, i, time\u001b[38;5;241m.\u001b[39mtime())\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py:378\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add scalar data to summary.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mscalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/summary.py:371\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscalar\u001b[39m(name, tensor, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, new_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, double_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Output a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmake_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    373\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    374\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/_convert_np.py:24\u001b[0m, in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_prepare_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but numpy array or torch tensor are expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/airi/lib/python3.12/site-packages/torch/utils/tensorboard/_convert_np.py:33\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "EPOCHS = 4000\n",
    "linear_schedule = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lambda i: min(1.0, i / (EPOCHS * len(train_data_loader)))\n",
    ")\n",
    "# tensorboard pytorch logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def estimate_loss():\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for x, y in test_data:\n",
    "#     for split in [\"train\", \"val\"]:\n",
    "#         losses = torch.zeros(eval_iters)\n",
    "#         for k in range(eval_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean()\n",
    "#     model.train()\n",
    "#     return out\n",
    "\n",
    "\n",
    "# training loop\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    for j, (X, y) in tqdm(\n",
    "        enumerate(train_data_loader),\n",
    "        unit=\"batch\",\n",
    "        total=len(train_data_loader),\n",
    "        desc=f\"Epoch {i}\",\n",
    "    ):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(X, y)\n",
    "        writer.add_scalar(\"Loss/train\", loss, time.time())\n",
    "        writer.add_scalar(\"Learning rate\", optimizer.param_groups[0][\"lr\"], time.time())\n",
    "        writer.add_scalar(\"Epoch\", i, time.time())\n",
    "        loss.backward()\n",
    "        linear_schedule.step()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in tqdm(\n",
    "            enumerate(test_data_loader),\n",
    "            unit=\"batch\",\n",
    "            total=len(test_data_loader),\n",
    "            desc=f\"Epoch {i}\",\n",
    "        ):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            logits, loss = model(X, y)\n",
    "            writer.add_scalar(\"Loss/val\", loss, time.time())\n",
    "\n",
    "            writer.add_scalar(\"Epoch\", i, time.time())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dark_room import DarkRoom\n",
    "from time import sleep\n",
    "\n",
    "from utils import print_grid\n",
    "\n",
    "model.eval()\n",
    "env = DarkRoom(size=9)\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "action_history = [[4.0, 4.0, 0, 0]]\n",
    "global_reward = 0\n",
    "\n",
    "\n",
    "def make_iteration(state, global_reward):\n",
    "    tokenized = torch.tensor([tokenizer([state])]).to(device)\n",
    "    policy = model(tokenized)\n",
    "    action = torch.softmax(policy[0], dim=-1).argmax().item()\n",
    "    # action = Categorical(policy).sample().item()\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "    action_history.append([state[0], state[1], action, global_reward + reward])\n",
    "\n",
    "    return state, global_reward + reward, done\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     state, reward, done = make_iteration(state)\n",
    "#     sleep(0.1)\n",
    "#     if done:\n",
    "#         print(f\"Goal reached in {i} steps\")\n",
    "#         print(f\"Action history: {action_history}\")\n",
    "#         print_grid(env.render())\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.0, 4.0, 0, 0],\n",
       " [3, 4, 2, -1],\n",
       " [2, 4, 2, -2],\n",
       " [1, 4, 2, -3],\n",
       " [0, 4, 2, -4],\n",
       " [0, 4, 2, -5],\n",
       " [0, 4, 2, -6],\n",
       " [0, 4, 2, -7],\n",
       " [0, 4, 2, -8],\n",
       " [0, 4, 2, -9],\n",
       " [0, 4, 2, -10],\n",
       " [0, 4, 2, -11],\n",
       " [0, 4, 2, -12],\n",
       " [0, 4, 2, -13],\n",
       " [0, 4, 2, -14],\n",
       " [0, 4, 2, -15],\n",
       " [0, 4, 2, -16],\n",
       " [0, 4, 2, -17],\n",
       " [0, 4, 2, -18],\n",
       " [0, 4, 2, -19],\n",
       " [0, 4, 2, -20],\n",
       " [0, 4, 2, -21],\n",
       " [0, 4, 2, -22],\n",
       " [0, 4, 2, -23],\n",
       " [0, 4, 2, -24],\n",
       " [0, 4, 2, -25],\n",
       " [0, 4, 2, -26],\n",
       " [0, 4, 2, -27],\n",
       " [0, 4, 2, -28],\n",
       " [0, 4, 2, -29],\n",
       " [0, 4, 2, -30],\n",
       " [0, 4, 2, -31]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[J\n",
      "[['X' ' ' 'G' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "state, global_reward, done = make_iteration(state, global_reward)\n",
    "print_grid(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2], -1, False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
